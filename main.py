
from openai import OpenAI
import time
from transformers import AutoTokenizer

openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"

client = OpenAI(
        api_key=openai_api_key,
        base_url=openai_api_base,
    )

# Get the list of available models served by vLLM
models = client.models.list()
model = models.data[0].id


# Load tokenizer for token counting,Language models (like LLaMA, Gemma, etc.) donâ€™t process raw text directly. They work on tokens, which are sub-word units generated by a tokenizer.
# Each model has its own tokenizer vocabulary and tokenization logic (e.g., SentencePiece for Gemma, Byte-Pair Encoding for GPT models). If you used a different tokenizer: you might miscount tokens, you might send mismatched input to the model.
tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)

def count_input_tokens_tokenizer(tokenizer, messages):
    '''Count input tokens based on user message using tokenizer'''
    full_text = "".join([m["content"] for m in messages])
    return len(tokenizer.encode(full_text))

def count_output_tokens(tokenizer, text):
    '''Count output tokens generated by model'''
    return len(tokenizer.encode(text))

def benchmarking(prompt):
    
    # Define the prompt as a chat-style message
    messages = [
        {"role": "user", "content": f"{prompt}"},
    ]

    
    # print("Tokenizer loaded in: ", time.time() - tt, " seconds")
    input_token_count = count_input_tokens_tokenizer(tokenizer, messages)
    start_time = time.time() # Timing starts before sending the request
    first_token_time = None # For measuring TTFT (time to first token)
    token_times = [] # Capture time of each token for latency analysis
    full_output = "" # Accumulate streamed output
    
    chat_completion = client.chat.completions.create(
        messages=messages,
        model=model,
        temperature=0.6,
        max_tokens=100,
        stream=True, # Streaming mode enabled for real-time token generation
    )
    
    # for chunk in chat_completion:
    #     if chunk.choices[0].delta.content is not None:
    #         print(chunk.choices[0].delta.content, end="")
    
    for chunk in chat_completion:
        now = time.time()
        if first_token_time is None:
            first_token_time = now # Capture time of first token
        delta = chunk.choices[0].delta
        content = delta.content or ""
        if content.strip(): # Only consider non-empty tokens
            token_times.append(now) # Record token arrival time
            full_output += content
            print(content, end="", flush=True) # Print token immediately

    end_time = time.time()  # End timing after full response is received
    
    # print("Output token count: ", count_output_tokens(tokenizer, full_output))
    op_tokens = count_output_tokens(tokenizer, full_output)
    ttft = first_token_time - start_time # Time to first token
    inter_token_latencies = [t2 - t1 for t1, t2 in zip(token_times, token_times[1:])]  # Inter-token latency
    avg_inter_token_latency = sum(inter_token_latencies) / len(inter_token_latencies) 
    
    # print("Average Inter-Token Latency: ", avg_inter_token_latency)
    total_duration = end_time - start_time  # Total latency
    # print("Total Latency: ", total_duration)
    tps = op_tokens / total_duration # Tokens per second
    tpm = (op_tokens / total_duration) * 60 # Tokens per minute
    # print("Tokens Per Second (TPS): ", tps)
    # print("Tokens Per Minute (TPM): ", tpm)
    # print("Output Tokens: ", op_tokens)
    
    result_dict = {
        "Input Token": input_token_count,
        "Output Token": op_tokens,
        "TPM": round(tpm, 2),
        "TPS": round(tps, 2),
        "TTFT": round(ttft, 2),
        "ITL": round(avg_inter_token_latency, 2),
        "latency": round(total_duration, 2),

    }
    
    print("\n")
    print(result_dict)
    

if __name__ == "__main__":
    benchmarking("Who won the world series in 2020?")